"""
Multi-Agent Pipeline Orchestrator
Coordinates the workflow between Build, Analyze, Fix, and Test agents
"""

import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from enum import Enum
import json
import time
import sys
import os

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.agents.build_agent import get_build_agent
from src.agents.analyze_agent import get_analyze_agent
from src.agents.fix_agent import get_fix_agent
from src.agents.test_agent import get_test_agent
from src.utils.github_client import get_github_client
from src.utils.results_webhook import ResultsAggregator, get_results_webhook_sender
from src.utils.terminal_websocket import get_terminal_websocket_manager

class PipelineStage(Enum):
    """Pipeline execution stages"""
    PENDING = "pending"
    BUILD = "build"
    ANALYZE = "analyze" 
    FIX = "fix"
    TEST = "test"
    COMPLETE = "complete"
    FAILED = "failed"

@dataclass
class PipelineContext:
    """Context shared between all agents"""
    pr_number: int
    repo_name: str
    branch: str
    files_changed: List[Dict[str, Any]]
    stage: PipelineStage
    results: Dict[str, Any]
    errors: List[str]
    start_time: float
    
class MultiAgentPipeline:
    """
    Orchestrates the multi-agent workflow:
    Build → Analyze → Fix → Test
    """
    
    def __init__(self):
        self.build_agent = get_build_agent()
        self.analyze_agent = get_analyze_agent()
        self.fix_agent = get_fix_agent()
        self.test_agent = get_test_agent()
        self.github_client = get_github_client()
        self.active_pipelines: Dict[str, PipelineContext] = {}
        self.websocket_manager = None
        self._pipeline_trigger_info: Dict[str, Dict[str, Any]] = {}  # Store trigger info for webhooks
        self.terminal_manager = get_terminal_websocket_manager()  # Terminal streaming manager
        
    def set_websocket_manager(self, manager):
        """Set the WebSocket manager for real-time updates"""
        self.websocket_manager = manager
        
    async def send_websocket_message(self, pipeline_id: str, message: dict):
        """Send WebSocket message if manager is available"""
        if self.websocket_manager:
            await self.websocket_manager.send_message(pipeline_id, message)
    
    async def start_terminal_streaming(self, pipeline_id: str, command: str, stage: str, cwd: str = None) -> str:
        """Start terminal streaming for a pipeline stage"""
        terminal_session_id = f"{pipeline_id}_{stage}_{int(time.time())}"
        
        # Notify pipeline WebSocket about terminal session start
        await self.send_websocket_message(pipeline_id, {
            "type": "terminal_session_start",
            "stage": stage,
            "terminal_session_id": terminal_session_id,
            "command": command,
            "timestamp": time.time()
        })
        
        # Start terminal streaming
        success = await self.terminal_manager.start_terminal_session(terminal_session_id, command, cwd)
        
        if success:
            print(f"🖥️  Terminal streaming started for {pipeline_id} - {stage}: {terminal_session_id}")
        else:
            print(f"❌ Failed to start terminal streaming for {pipeline_id} - {stage}")
        
        return terminal_session_id if success else None
    
    async def is_ai_generated_commit(self, repo_name: str, commit_sha: str) -> bool:
        """
        Check if a commit was generated by AI agents to prevent recursion
        
        Args:
            repo_name: Repository name (owner/repo)
            commit_sha: Commit SHA to check
            
        Returns:
            True if the commit was generated by AI, False otherwise
        """
        try:
            # Get commit details from GitHub
            commit = self.github_client.get_commit(repo_name, commit_sha)
            if not commit:
                print(f"⚠️ Could not fetch commit {commit_sha} for recursion check")
                return False
            
            commit_message = commit.commit.message
            
            # Check for AI-generated commit markers
            ai_markers = [
                "[skip-pipeline]",
                "🤖 AI Fix:",
                "🤖 AI Test:",
                "🤖 AI Refactor:",
                "[ai-generated]",
                "[hackademia-ai]"
            ]
            
            for marker in ai_markers:
                if marker in commit_message:
                    print(f"🚫 Detected AI commit marker '{marker}' in: {commit_message[:100]}")
                    return True
            
            # Check if committer is the AI bot (if using GitHub App)
            committer_name = commit.commit.committer.name if commit.commit.committer else ""
            if committer_name and "hackademia" in committer_name.lower():
                print(f"🚫 Detected AI committer: {committer_name}")
                return True
            
            print(f"✅ Commit {commit_sha[:8]} is not AI-generated: {commit_message[:50]}...")
            return False
            
        except Exception as e:
            print(f"❌ Error checking if commit {commit_sha} is AI-generated: {str(e)}")
            # In case of error, assume it's not AI-generated to be safe
            return False
    
    async def start_pipeline(self, pr_number: int, repo_name: str, trigger_info: Optional[Dict[str, Any]] = None) -> str:
        """
        Start the multi-agent pipeline for a PR
        
        Args:
            pr_number: Pull request number
            repo_name: Repository name (owner/repo format)
            trigger_info: Optional information about what triggered this pipeline
        
        Returns:
            pipeline_id: Unique identifier for tracking
        """
        pipeline_id = f"{repo_name}_{pr_number}_{int(time.time())}"
        
        # Store trigger info for webhook later
        if trigger_info is None:
            trigger_info = {
                "trigger_type": "manual",
                "triggered_by": "system", 
                "event_type": "unknown",
                "timestamp": time.time()
            }
        self._pipeline_trigger_info = {pipeline_id: trigger_info}
        
        try:
            print(f"🚀 Starting pipeline {pipeline_id}")
            
            # 🚫 RECURSION PREVENTION: Check if there's already an active pipeline for this PR
            existing_pipeline = self._get_active_pipeline_for_pr(repo_name, pr_number)
            if existing_pipeline:
                print(f"🚫 Skipping - pipeline already active for PR #{pr_number}: {existing_pipeline}")
                raise Exception(f"Pipeline already running for PR #{pr_number}")
            
            # Get PR information
            pr = self.github_client.get_pull_request(repo_name, pr_number)
            if not pr:
                raise Exception(f"Could not access PR #{pr_number}")
            
            # Get changed files
            files_changed = self.github_client.get_pr_files(repo_name, pr_number)
            
            # Initialize pipeline context
            context = PipelineContext(
                pr_number=pr_number,
                repo_name=repo_name,
                branch=pr.head.ref,
                files_changed=files_changed,
                stage=PipelineStage.PENDING,
                results={},
                errors=[],
                start_time=time.time()
            )
            
            self.active_pipelines[pipeline_id] = context
            
            # Send pipeline_start WebSocket message
            await self.send_websocket_message(pipeline_id, {
                "type": "pipeline_start",
                "pipeline_id": pipeline_id,
                "pr_number": pr_number,
                "repo_name": repo_name,
                "branch": context.branch,
                "stages": ["build", "analyze", "fix", "test"]
            })
            
            # Start async pipeline execution
            asyncio.create_task(self._execute_pipeline(pipeline_id))
            
            return pipeline_id
            
        except Exception as e:
            error_msg = f"Failed to start pipeline: {str(e)}"
            print(f"❌ {error_msg}")
            raise Exception(error_msg)
    
    def _get_active_pipeline_for_pr(self, repo_name: str, pr_number: int) -> Optional[str]:
        """Check if there's already an active pipeline for this PR"""
        for pipeline_id, context in self.active_pipelines.items():
            if (context.repo_name == repo_name and 
                context.pr_number == pr_number and 
                context.stage not in [PipelineStage.COMPLETE, PipelineStage.FAILED]):
                return pipeline_id
        return None
    
    async def _execute_pipeline(self, pipeline_id: str):
        """Execute the complete multi-agent pipeline"""
        context = self.active_pipelines[pipeline_id]
        
        try:
            print(f"🔄 Executing pipeline {pipeline_id}")
            
            # Get pipeline_id for WebSocket messages
            pipeline_id = f"{context.repo_name}_{context.pr_number}_{int(context.start_time)}"
            
            # Stage 1: Build Agent (REAL)
            await self._run_build_stage(context)
            
            # Stage 2: Analyze Agent (REAL) - Only if build succeeded
            if context.stage != PipelineStage.FAILED:
                await self._run_analyze_stage(context)
            
            # Stage 3: Fix Agent (NEW) - Only if analyze found issues
            if context.stage != PipelineStage.FAILED and context.results.get('analyze', {}).get('total_issues', 0) > 0:
                await self._run_fix_stage(context, pipeline_id)
            
            # Stage 4: Test Agent (Complete 3-phase implementation)
            if context.stage != PipelineStage.FAILED:
                await self._run_test_stage_complete(context, pipeline_id)
            
            # Complete pipeline
            context.stage = PipelineStage.COMPLETE
            
            # Send pipeline_complete message
            total_duration = time.time() - context.start_time
            await self.send_websocket_message(pipeline_id, {
                "type": "pipeline_complete",
                "status": "success",
                "total_duration": round(total_duration, 2),
                "summary": {
                    "build": {"status": "success" if context.results.get('build', {}).get('success', False) else "failed"},
                    "analyze": {"status": "success" if context.results.get('analyze', {}).get('success', False) else "failed", "issues_found": context.results.get('analyze', {}).get('total_issues', 0)},
                    "fix": {"status": "success" if context.results.get('fix', {}).get('success', False) else "skipped", "fixes_applied": context.results.get('fix', {}).get('fixes_applied', 0)},
                    "test": {
                        "status": "success" if context.results.get('test', {}).get('success', False) else "failed", 
                        "functions_discovered": context.results.get('test', {}).get('functions_discovered', 0), 
                        "questions_generated": context.results.get('test', {}).get('questions_generated', 0),
                        "tests_generated": context.results.get('test', {}).get('tests_generated', 0),
                        "tests_executed": context.results.get('test', {}).get('tests_executed', 0),
                        "tests_passed": context.results.get('test', {}).get('tests_passed', 0),
                        "execution_success": context.results.get('test', {}).get('execution_success', False)
                    }
                }
            })
            
            # 🚀 NEW: Send comprehensive results via WebSocket
            await self._send_comprehensive_results_websocket(context, pipeline_id)
            
            await self._post_results_to_pr(context)
            
            print(f"✅ Pipeline {pipeline_id} completed successfully")
            
            # Clean up completed pipeline from active pipelines
            if pipeline_id in self.active_pipelines:
                del self.active_pipelines[pipeline_id]
                print(f"🧹 Cleaned up pipeline {pipeline_id} from active pipelines")
            
        except Exception as e:
            context.stage = PipelineStage.FAILED
            context.errors.append(str(e))
            
            # Send error message  
            total_duration = time.time() - context.start_time
            await self.send_websocket_message(pipeline_id, {
                "type": "error",
                "stage": "pipeline",
                "message": f"Pipeline failed: {str(e)}",
                "error_code": "PIPELINE_FAILED",
                "details": str(e)
            })
            
            await self.send_websocket_message(pipeline_id, {
                "type": "pipeline_complete", 
                "status": "failed",
                "total_duration": round(total_duration, 2),
                "summary": {
                    "build": {"status": "failed"},
                    "analyze": {"status": "skipped"},
                    "fix": {"status": "skipped"},
                    "test": {"status": "skipped"}
                }
            })
            
            print(f"❌ Pipeline {pipeline_id} failed: {str(e)}")
            
            # 🚀 NEW: Send comprehensive results via WebSocket (even for failures)
            await self._send_comprehensive_results_websocket(context, pipeline_id)
            
            await self._post_results_to_pr(context)
            
            # Clean up failed pipeline from active pipelines  
            if pipeline_id in self.active_pipelines:
                del self.active_pipelines[pipeline_id]
                print(f"🧹 Cleaned up failed pipeline {pipeline_id} from active pipelines")
    
    async def _run_build_stage(self, context: PipelineContext):
        """Run Build Agent stage"""
        print(f"🔨 Running Build stage for {context.repo_name}#{context.pr_number}")
        context.stage = PipelineStage.BUILD
        
        # Get pipeline_id for WebSocket messages
        pipeline_id = f"{context.repo_name}_{context.pr_number}_{int(context.start_time)}"
        
        # Send stage_start message
        await self.send_websocket_message(pipeline_id, {
            "type": "stage_start",
            "stage": "build",
            "stage_index": 1,
            "message": f"Starting build stage for PR #{context.pr_number}..."
        })
        
        # Start terminal streaming for build stage
        build_terminal_session = await self.start_terminal_streaming(
            pipeline_id, 
            f"echo 'Starting build process for {context.repo_name}#{context.pr_number}'", 
            "build"
        )
        
        stage_start_time = time.time()
        
        # Use new PR branch cloning and building
        build_result = await self.build_agent.build_pr_branch(
            repo_name=context.repo_name,
            branch=context.branch,
            pr_number=context.pr_number,
            progress_callback=lambda msg: self.send_websocket_message(pipeline_id, msg)
        )
        
        # Store results
        context.results['build'] = {
            "success": build_result.success,
            "metadata": build_result.metadata,
            "errors": build_result.errors,
            "warnings": build_result.warnings,
            "dependencies": build_result.dependencies,
            "file_info": build_result.file_info,
            "build_logs": build_result.build_logs,
            "agent_context": self.build_agent.prepare_context_for_agents(build_result, repo_name=context.repo_name, pr_number=context.pr_number)
        }
        
        stage_duration = time.time() - stage_start_time
        
        # Send stage_complete message
        await self.send_websocket_message(pipeline_id, {
            "type": "stage_complete",
            "stage": "build",
            "status": "success" if build_result.success else "failed",
            "duration": round(stage_duration, 2),
            "results": {
                "build_logs": build_result.build_logs[-5:] if build_result.build_logs else [],
                "errors": build_result.errors,
                "metadata": {
                    "files_analyzed": len(build_result.file_info),
                    "dependencies_found": len(build_result.dependencies),
                    "project_type": build_result.metadata.get("project_type", "unknown")
                }
            }
        })
        
        print(f"✅ Build stage completed - Success: {build_result.success}")
        if build_result.build_logs:
            print("📋 Build logs:")
            for log in build_result.build_logs[-3:]:  # Show last 3 logs
                print(f"   {log}")
        
        if not build_result.success:
            print(f"❌ Build errors: {build_result.errors}")
            # Don't continue to other stages if build failed
            context.stage = PipelineStage.FAILED
            context.errors.extend(build_result.errors)
    
    async def _run_analyze_stage(self, context: PipelineContext):
        """Run Analyze Agent stage with real AI analysis"""
        print(f"🔍 Running AI Analysis stage for {context.repo_name}#{context.pr_number}")
        context.stage = PipelineStage.ANALYZE
        
        # Get pipeline_id for WebSocket messages
        pipeline_id = f"{context.repo_name}_{context.pr_number}_{int(context.start_time)}"
        
        # Send stage_start message
        await self.send_websocket_message(pipeline_id, {
            "type": "stage_start",
            "stage": "analyze",
            "stage_index": 2,
            "message": f"🧠 Starting AI-powered code analysis..."
        })
        
        # Start terminal streaming for analyze stage
        analyze_terminal_session = await self.start_terminal_streaming(
            pipeline_id, 
            f"echo 'Analyzing code changes in {context.repo_name}#{context.pr_number}'", 
            "analyze"
        )
        
        stage_start_time = time.time()
        
        try:
            # Get PR diff data for analysis
            diff_data = self.github_client.get_pr_diff_content(context.repo_name, context.pr_number)
            
            if not diff_data:
                raise Exception("Could not retrieve PR diff data")
            
            # Get build context
            build_context = context.results.get('build', {})
            
            # Run AI analysis
            analysis_result = await self.analyze_agent.analyze_pr_diff(
                diff_data=diff_data,
                build_context=build_context,
                progress_callback=lambda msg: self.send_websocket_message(pipeline_id, msg)
            )
            
            # Store results
            context.results['analyze'] = {
                "success": analysis_result.success,
                "vulnerabilities": analysis_result.vulnerabilities,
                "security_issues": analysis_result.security_issues,
                "quality_issues": analysis_result.quality_issues,
                "recommendations": analysis_result.recommendations,
                "overall_risk": analysis_result.overall_risk,
                "files_analyzed": analysis_result.files_analyzed,
                "total_issues": analysis_result.total_issues,
                "confidence_scores": analysis_result.confidence_scores,
                "fix_agent_context": self.analyze_agent.prepare_context_for_fix_agent(analysis_result)
            }
            
            stage_duration = time.time() - stage_start_time
            
            # Send stage_complete message with comprehensive analysis results
            await self.send_websocket_message(pipeline_id, {
                "type": "stage_complete",
                "stage": "analyze",
                "status": "success" if analysis_result.success else "failed",
                "duration": round(stage_duration, 2),
                "results": {
                    "files_analyzed": analysis_result.files_analyzed,
                    "total_issues": analysis_result.total_issues,
                    "vulnerabilities": [
                        {
                            "type": vuln.get("type", "UNKNOWN"),
                            "severity": vuln.get("severity", "UNKNOWN"), 
                            "file": vuln.get("file", "unknown"),
                            "line": vuln.get("line_number", 0),
                            "description": vuln.get("description", "No description")[:100] + "..." if len(vuln.get("description", "")) > 100 else vuln.get("description", "")
                        }
                        for vuln in analysis_result.vulnerabilities[:5]  # Limit to top 5 for WebSocket message size
                    ],
                    "security_issues": [
                        {
                            "type": issue.get("type", "UNKNOWN"),
                            "severity": issue.get("severity", "UNKNOWN"),
                            "file": issue.get("file", "unknown"), 
                            "line": issue.get("line_number", 0)
                        }
                        for issue in analysis_result.security_issues[:5]
                    ],
                    "quality_issues": [
                        {
                            "type": issue.get("type", "UNKNOWN"),
                            "severity": issue.get("severity", "UNKNOWN"),
                            "file": issue.get("file", "unknown"),
                            "line": issue.get("line_number", 0)
                        }
                        for issue in analysis_result.quality_issues[:5]
                    ],
                    "recommendations": analysis_result.recommendations[:5],
                    "next_stage": "fix" if analysis_result.total_issues > 0 else "test",
                    "metadata": {
                        "mcp_questions_asked": getattr(analysis_result, 'mcp_questions_asked', 'unknown'),
                        "context_gathered": getattr(analysis_result, 'context_gathered', 'unknown'),
                        "analysis_time": round(stage_duration, 2),
                        "ai_model": "Gemini 2.5 Flash",
                        "overall_risk": analysis_result.overall_risk,
                        "confidence_scores": analysis_result.confidence_scores
                    }
                }
            })
            
            print(f"✅ Analysis stage completed - {analysis_result.total_issues} issues found")
            print(f"🎯 Risk Level: {analysis_result.overall_risk}")
            
            if not analysis_result.success:
                print(f"⚠️ Analysis completed with warnings")
                context.stage = PipelineStage.FAILED
                context.errors.append("Analysis stage had issues")
        
        except Exception as e:
            print(f"❌ Analysis stage failed: {str(e)}")
            stage_duration = time.time() - stage_start_time
            
            # Send error message
            await self.send_websocket_message(pipeline_id, {
                "type": "error",
                "stage": "analyze",
                "message": f"Analysis failed: {str(e)}",
                "error_code": "ANALYZE_FAILED",
                "details": str(e)
            })
            
            # Send stage_complete with failure
            await self.send_websocket_message(pipeline_id, {
                "type": "stage_complete",
                "stage": "analyze", 
                "status": "failed",
                "duration": round(stage_duration, 2),
                "results": {"error": str(e)}
            })
            
            context.stage = PipelineStage.FAILED
            context.errors.append(f"Analysis failed: {str(e)}")
            
            # Store failed result
            context.results['analyze'] = {
                "success": False,
                "error": str(e),
                "total_issues": 0,
                "message": f"AI analysis failed: {str(e)}"
            }
    
    async def _run_fix_stage(self, context: PipelineContext, pipeline_id: str):
        """Run Fix Agent stage for high-confidence issues"""
        print(f"🔧 Running AI Fix stage for {context.repo_name}#{context.pr_number}")
        context.stage = PipelineStage.FIX
        
        # Send stage_start message
        await self.send_websocket_message(pipeline_id, {
            "type": "stage_start",
            "stage": "fix",
            "stage_index": 3,
            "message": f"🔧 Starting AI-powered code fixes..."
        })
        
        # Start terminal streaming for fix stage
        fix_terminal_session = await self.start_terminal_streaming(
            pipeline_id, 
            f"echo 'Applying AI fixes to {context.repo_name}#{context.pr_number}'", 
            "fix"
        )
        
        stage_start_time = time.time()
        
        try:
            # Get analysis results
            analysis_result = context.results.get('analyze', {})
            
            if not analysis_result.get('success', False):
                print("⏭️ Skipping fix stage - analysis failed")
                return
            
            # Convert analysis results to AnalysisResult object for Fix Agent
            from src.agents.analyze_agent import AnalysisResult
            analysis_obj = AnalysisResult(
                success=analysis_result.get('success', False),
                vulnerabilities=analysis_result.get('vulnerabilities', []),
                security_issues=analysis_result.get('security_issues', []),
                quality_issues=analysis_result.get('quality_issues', []),
                recommendations=analysis_result.get('recommendations', []),
                overall_risk=analysis_result.get('overall_risk', 'LOW'),
                files_analyzed=analysis_result.get('files_analyzed', 0),
                total_issues=analysis_result.get('total_issues', 0),
                confidence_scores=analysis_result.get('confidence_scores', {})
            )
            
            # Run Fix Agent
            fix_result = await self.fix_agent.apply_fixes(
                analysis_result=analysis_obj,
                repo_name=context.repo_name,
                branch=context.branch,
                progress_callback=lambda msg: self.send_websocket_message(pipeline_id, msg)
            )
            
            # Store results
            context.results['fix'] = {
                "success": fix_result.success,
                "fixes_applied": fix_result.fixes_applied,
                "files_modified": fix_result.files_modified,
                "commits_made": fix_result.commits_made,
                "fixes_summary": fix_result.fixes_summary,
                "errors": fix_result.errors,
                "duration": fix_result.duration,
                "message": f"Applied {fix_result.fixes_applied} fixes to {fix_result.files_modified} files"
            }
            
            print(f"✅ Fix stage completed - {fix_result.fixes_applied} fixes applied")
            
            if not fix_result.success:
                print(f"⚠️ Fix stage completed with errors: {fix_result.errors}")
        
        except Exception as e:
            print(f"❌ Fix stage failed: {str(e)}")
            stage_duration = time.time() - stage_start_time
            
            # Send error message
            await self.send_websocket_message(pipeline_id, {
                "type": "error",
                "stage": "fix",
                "message": f"Fix stage failed: {str(e)}",
                "error_code": "FIX_FAILED",
                "details": str(e)
            })
            
            # Send stage_complete with failure
            await self.send_websocket_message(pipeline_id, {
                "type": "stage_complete",
                "stage": "fix",
                "status": "failed",
                "duration": round(stage_duration, 2),
                "results": {"error": str(e)}
            })
            
            context.stage = PipelineStage.FAILED
            context.errors.append(f"Fix failed: {str(e)}")
            
            # Store failed result
            context.results['fix'] = {
                "success": False,
                "error": str(e),
                "fixes_applied": 0,
                "message": f"AI fixing failed: {str(e)}"
            }
    
    async def _run_test_stage_complete(self, context: PipelineContext, pipeline_id: str):
        """Run complete Test Agent: Phase 1 (function discovery) + Phase 2 (CodeRM-8B test generation) + Phase 3 (execution)"""
        print(f"🧪 Running Complete Test Stage for {context.repo_name}#{context.pr_number}")
        context.stage = PipelineStage.TEST
        
        try:
            # Send stage start message
            await self.send_websocket_message(pipeline_id, {
                "type": "stage_start",
                "stage": "test",
                "stage_index": 4,
                "message": f"Starting test generation stage for PR #{context.pr_number}...",
                "details": {
                    "phase": "starting",
                    "description": "AI-powered unit test generation with CodeRM-8B"
                }
            })
            
            # Start terminal streaming for test stage
            test_terminal_session = await self.start_terminal_streaming(
                pipeline_id, 
                f"echo 'Generating and running tests for {context.repo_name}#{context.pr_number}'", 
                "test"
            )
            
            stage_start_time = time.time()
            
            # Get diff data and fix results from previous stages
            diff_data = context.results.get('build', {}).get('agent_context', {})
            if not diff_data or not diff_data.get('changed_files'):
                print(f"⚠️ No diff data from build agent, fetching from GitHub directly...")
                # Fallback: Get diff data from GitHub directly
                diff_data = self.github_client.get_pr_diff_content(context.repo_name, context.pr_number)
                print(f"📊 Fetched diff data: {len(diff_data.get('changed_files', []))} changed files")
            else:
                print(f"📊 Using build agent diff data: {len(diff_data.get('changed_files', []))} changed files")
            
            fix_results = context.results.get('fix', {})
            
            # Phase 1: Function discovery and question generation
            print(f"🔍 Starting Phase 1: Function Discovery...")
            test_result_phase1 = await self.test_agent.run_test_stage_phase1(
                diff_data=diff_data,
                fix_results=fix_results,
                repo_name=context.repo_name,
                branch=context.branch,
                progress_callback=lambda msg: self.send_websocket_message(pipeline_id, msg)
            )
            
            if not test_result_phase1.success:
                print(f"❌ Test Agent Phase 1 failed: {test_result_phase1.errors}")
                
                # Store failed results
                context.results['test'] = {
                    "success": False,
                    "phase": "phase1_failed",
                    "functions_discovered": 0,
                    "questions_generated": 0,
                    "tests_generated": 0,
                    "errors": test_result_phase1.errors,
                    "duration": time.time() - stage_start_time
                }
                
                await self.send_websocket_message(pipeline_id, {
                    "type": "stage_complete",
                    "stage": "test",
                    "status": "failed",
                    "duration": round(time.time() - stage_start_time, 2),
                    "message": f"❌ Test generation failed in Phase 1: {'; '.join(test_result_phase1.errors)}"
                })
                return
            
            print(f"✅ Phase 1 completed: {test_result_phase1.functions_discovered} functions, {test_result_phase1.questions_generated} questions")
            
            # Initialize generated_tests to prevent reference error
            generated_tests = []
            
            # Phase 2: CodeRM-8B test generation
            if test_result_phase1.functions_with_questions:
                print(f"🤖 Starting Phase 2: CodeRM-8B Test Generation...")
                
                await self.send_websocket_message(pipeline_id, {
                    "type": "status_update",
                    "stage": "test",
                    "status": "in_progress",
                    "message": f"🤖 Phase 2: Generating tests with CodeRM-8B for {len(test_result_phase1.functions_with_questions)} functions...",
                    "progress": 65,
                    "details": {
                        "phase": "test_generation",
                        "functions_to_test": len(test_result_phase1.functions_with_questions)
                    }
                })
                
                generated_tests = await self.test_agent.run_test_stage_phase2(
                    function_questions=test_result_phase1.functions_with_questions,
                    progress_callback=lambda msg: self.send_websocket_message(pipeline_id, msg)
                )
                
                print(f"✅ Phase 2 completed: {len(generated_tests)} tests generated")
            
            # Phase 3: Test execution
            if generated_tests:
                print(f"🧪 Starting Phase 3: Test Execution...")
                
                await self.send_websocket_message(pipeline_id, {
                    "type": "status_update",
                    "stage": "test",
                    "status": "in_progress",
                    "message": f"🧪 Phase 3: Executing {len(generated_tests)} generated unit tests...",
                    "progress": 80,
                    "details": {
                        "phase": "test_execution",
                        "tests_to_execute": len(generated_tests)
                    }
                })
                
                execution_result = await self.test_agent.run_test_stage_phase3(
                    generated_tests=generated_tests,
                    progress_callback=lambda msg: self.send_websocket_message(pipeline_id, msg)
                )
                
                print(f"✅ Phase 3 completed: Tests executed with {execution_result.success} success")
                
                # Update results with execution data
                tests_executed = len(generated_tests)
                tests_passed = tests_executed if execution_result.success else 0
                
                execution_summary = f"Executed {tests_executed} test files"
                
            else:
                print(f"⚠️ No generated tests to execute, skipping Phase 3")
                execution_result = None
                tests_executed = 0
                tests_passed = 0
                execution_summary = "No tests to execute"
            
            # Store complete results (all 3 phases)
            context.results['test'] = {
                "success": True,
                "phase": "complete_with_execution",
                "functions_discovered": test_result_phase1.functions_discovered,
                "questions_generated": test_result_phase1.questions_generated,
                "tests_generated": len(generated_tests),
                "tests_executed": tests_executed,
                "tests_passed": tests_passed,
                "execution_success": execution_result.success if execution_result else False,
                "functions_with_questions": [
                    {
                        "filename": fq.function.filename,
                        "function_name": fq.function.function_name,
                        "question": fq.question,
                        "start_line": fq.function.start_line,
                        "end_line": fq.function.end_line,
                        "is_class_method": fq.function.is_class_method,
                        "class_name": fq.function.class_name
                    }
                    for fq in test_result_phase1.functions_with_questions
                ],
                "generated_tests": [
                    {
                        "filename": test.function.filename,
                        "function_name": test.function.function_name,
                        "test_name": test.test_name,
                        "test_code": test.test_code,
                        "confidence_score": test.confidence_score,
                        "question": test.question
                    }
                    for test in generated_tests
                ],
                "errors": test_result_phase1.errors,
                "duration": time.time() - stage_start_time,
                "message": f"Complete test pipeline: {test_result_phase1.functions_discovered} functions → {len(generated_tests)} tests → {execution_summary}",
                "execution_summary": execution_summary
            }
            
            # Send completion message with all phases
            await self.send_websocket_message(pipeline_id, {
                "type": "stage_complete",
                "stage": "test",
                "status": "success",
                "duration": round(time.time() - stage_start_time, 2),
                "message": f"✅ Complete test pipeline finished: {len(generated_tests)} tests generated and executed",
                "details": {
                    "functions_discovered": test_result_phase1.functions_discovered,
                    "questions_generated": test_result_phase1.questions_generated,
                    "tests_generated": len(generated_tests),
                    "tests_executed": tests_executed,
                    "tests_passed": tests_passed,
                    "execution_success": execution_result.success if execution_result else False,
                    "phase": "complete_with_execution"
                }
            })
            
            print(f"✅ Test stage completed successfully")
            
        except Exception as e:
            error_msg = f"Test Stage failed: {str(e)}"
            print(f"❌ {error_msg}")
            
            # Store failed results
            context.results['test'] = {
                "success": False,
                "phase": "failed",
                "functions_discovered": 0,
                "questions_generated": 0,
                "tests_generated": 0,
                "errors": [error_msg],
                "duration": time.time() - stage_start_time
            }
            
            await self.send_websocket_message(pipeline_id, {
                "type": "stage_complete",
                "stage": "test",
                "status": "failed",
                "duration": round(time.time() - stage_start_time, 2),
                "message": error_msg
            })
            
            print(f"❌ Test stage failed: {str(e)}")
    
    async def _post_results_to_pr(self, context: PipelineContext):
        """Post comprehensive results to PR"""
        try:
            # Generate results summary
            duration = time.time() - context.start_time
            
            comment = self._generate_results_comment(context, duration)
            
            # Post to GitHub
            success = self.github_client.create_comment(
                context.repo_name, 
                context.pr_number, 
                comment
            )
            
            if success:
                print(f"✅ Results posted to PR #{context.pr_number}")
            else:
                print(f"❌ Failed to post results to PR")
                
        except Exception as e:
            print(f"❌ Error posting results: {str(e)}")
    
    def _generate_results_comment(self, context: PipelineContext, duration: float) -> str:
        """Generate comprehensive markdown comment for PR with detailed results"""
        status_emoji = "✅" if context.stage == PipelineStage.COMPLETE else "❌"
        
        # Get comprehensive results for detailed comment
        pipeline_id = f"{context.repo_name}_{context.pr_number}_{int(context.start_time)}"
        trigger_info = self._pipeline_trigger_info.get(pipeline_id, {
            "trigger_type": "webhook", "triggered_by": "github", "event_type": "pull_request", "timestamp": time.time()
        })
        
        try:
            comprehensive_results = ResultsAggregator.aggregate_pipeline_results(context, pipeline_id, trigger_info)
        except:
            # Fallback to basic comment if aggregation fails
            return self._generate_basic_results_comment(context, duration)
        
        comment = f"""
# {status_emoji} Hackademia AI Pipeline Results

**Pipeline ID**: `{pipeline_id}`  
**Duration**: {duration:.2f} seconds  
**Status**: {comprehensive_results.pipeline_status.value}  
**Success Rate**: {comprehensive_results.success_rate:.1f}%

---

## 🔨 Build Agent Results
- **Status**: {'✅ Success' if comprehensive_results.build_results.success else '❌ Failed'}
- **Files Downloaded**: {comprehensive_results.build_results.files_downloaded}
- **File Types**: {', '.join(comprehensive_results.build_results.file_types_processed)}
- **Duration**: {comprehensive_results.build_results.duration:.2f}s
"""
        
        if comprehensive_results.build_results.build_errors:
            comment += f"\n**Build Errors:**\n"
            for error in comprehensive_results.build_results.build_errors[:3]:
                comment += f"- ❌ {error}\n"
        
        comment += f"""
## 🔍 Analyze Agent Results
- **Status**: {'✅ Success' if comprehensive_results.analysis_results.success else '❌ Failed'}
- **Files Analyzed**: {comprehensive_results.analysis_results.files_analyzed}
- **Issues Found**: {comprehensive_results.analysis_results.total_issues}
- **Overall Risk**: {comprehensive_results.analysis_results.overall_risk_level}
- **AI Confidence**: {comprehensive_results.analysis_results.ai_confidence_score:.2f}
- **Duration**: {comprehensive_results.analysis_results.duration:.2f}s

### 🚨 Severity Breakdown
"""
        
        for severity, count in comprehensive_results.analysis_results.severity_breakdown.items():
            severity_emoji = {"critical": "🔴", "high": "🟠", "medium": "🟡", "low": "🟢"}.get(severity.lower(), "⚪")
            comment += f"- {severity_emoji} **{severity.title()}**: {count}\n"
        
        comment += f"\n### 📊 Issue Categories\n"
        for category, count in comprehensive_results.analysis_results.categories_breakdown.items():
            comment += f"- **{category.title()}**: {count}\n"
        
        if comprehensive_results.analysis_results.vulnerabilities:
            comment += f"\n### 🚨 Top Vulnerabilities\n"
            for vuln in comprehensive_results.analysis_results.vulnerabilities[:5]:  # Top 5
                severity_emoji = {"critical": "🔴", "high": "🟠", "medium": "🟡", "low": "🟢"}.get(vuln.severity.lower(), "⚪")
                comment += f"- {severity_emoji} **{vuln.type}** (Line {vuln.line_number}): {vuln.description[:100]}{'...' if len(vuln.description) > 100 else ''}\n"
                comment += f"  - File: `{vuln.file_path}`\n"
                comment += f"  - Confidence: {vuln.confidence_score:.2f}\n\n"
        
        if comprehensive_results.analysis_results.recommendations:
            comment += f"### 💡 AI Recommendations\n"
            for rec in comprehensive_results.analysis_results.recommendations[:3]:
                comment += f"- {rec}\n"

        comment += f"""
## 🔧 Fix Agent Results  
- **Status**: {'✅ Success' if comprehensive_results.fix_results.success else '❌ Failed'}
- **Files Modified**: {comprehensive_results.fix_results.files_modified}
- **Functions Fixed**: {len(comprehensive_results.fix_results.functions_fixed)}
- **Lines Changed**: {comprehensive_results.fix_results.total_lines_changed}
- **Avg Confidence**: {comprehensive_results.fix_results.fix_confidence_average:.2f}
- **Duration**: {comprehensive_results.fix_results.duration:.2f}s
"""
        
        if comprehensive_results.fix_results.commit_sha:
            comment += f"- **Commit**: [`{comprehensive_results.fix_results.commit_sha[:8]}`](../../commit/{comprehensive_results.fix_results.commit_sha})\n"
        
        if comprehensive_results.fix_results.functions_fixed:
            comment += f"\n### 🛠️ Functions Fixed\n"
            for fix in comprehensive_results.fix_results.functions_fixed[:5]:  # Top 5
                comment += f"- **{fix.function_name}** in `{fix.file_path}`\n"
                comment += f"  - Fix Type: {fix.fix_type}\n"
                comment += f"  - Description: {fix.description}\n"
                comment += f"  - Confidence: {fix.confidence_score:.2f}\n\n"

        comment += f"""
## 🧪 Test Agent Results
- **Status**: {'✅ Success' if comprehensive_results.test_results.success else '❌ Failed'}
- **Functions Discovered**: {comprehensive_results.test_results.functions_discovered}
- **Tests Generated**: {comprehensive_results.test_results.tests_generated}
- **Tests Executed**: {comprehensive_results.test_results.tests_executed}
- **Tests Passed**: {comprehensive_results.test_results.tests_passed}
- **Tests Failed**: {comprehensive_results.test_results.tests_failed}
- **Coverage**: {comprehensive_results.test_results.test_coverage_percentage:.1f}%
- **Duration**: {comprehensive_results.test_results.duration:.2f}s
"""
        
        if comprehensive_results.test_results.test_functions:
            comment += f"\n### 🧪 Test Execution Details\n"
            for test in comprehensive_results.test_results.test_functions[:5]:  # Top 5
                status_emoji = {"passed": "✅", "failed": "❌", "skipped": "⏭️"}.get(test.status, "⚪")
                comment += f"- {status_emoji} **{test.test_name}** ({test.execution_time:.2f}s)\n"
                comment += f"  - Function: `{test.function_name}`\n"
                comment += f"  - File: `{test.file_path}`\n"
                if test.error_message:
                    comment += f"  - Error: {test.error_message[:100]}{'...' if len(test.error_message) > 100 else ''}\n"
                comment += "\n"

        comment += f"""
## 📊 Resource Metrics
- **Total Processing Time**: {comprehensive_results.total_duration:.2f}s
- **API Calls**: {comprehensive_results.resource_metrics.total_api_calls}
- **Peak Memory**: {comprehensive_results.resource_metrics.memory_usage_peak:.1f}MB

## 🎯 Summary
- **Pipeline Status**: {comprehensive_results.pipeline_status.value.upper()}
- **Success Rate**: {comprehensive_results.success_rate:.1f}%
- **Issues Identified**: {len(comprehensive_results.analysis_results.vulnerabilities)}
- **Fixes Applied**: {len(comprehensive_results.fix_results.functions_fixed)}
- **Tests Generated**: {comprehensive_results.test_results.tests_generated}

---
*Powered by Hackademia Multi-Agent AI Pipeline* 🚀  
*Pipeline ID: `{pipeline_id}`*
"""
        
        return comment
    
    def _generate_basic_results_comment(self, context: PipelineContext, duration: float) -> str:
        """Generate basic markdown comment as fallback"""
        status_emoji = "✅" if context.stage == PipelineStage.COMPLETE else "❌"
        
        comment = f"""
# {status_emoji} Hackademia AI Pipeline Results

**Pipeline ID**: `{context.repo_name}_{context.pr_number}`  
**Duration**: {duration:.2f} seconds  
**Status**: {context.stage.value}

## 🔨 Build Agent Results
"""
        
        if 'build' in context.results:
            build = context.results['build']
            meta = build.get('metadata', {}) or {}
            comment += f"""
- **Status**: {'✅ Success' if build.get('success') else '❌ Failed'}
- **Files Analyzed**: {meta.get('total_files', 0)}
- **Functions Found**: {meta.get('total_functions', 0)}
- **Classes Found**: {meta.get('total_classes', 0)}
- **Dependencies**: {meta.get('unique_dependencies', 0)}
"""
            
            if build.get('errors'):
                comment += f"\n**Build Errors:**\n"
                for error in build.get('errors', []):
                    comment += f"- ❌ {error}\n"
        
        comment += f"""
## 🔍 Analyze Agent Results
"""
        
        if 'analyze' in context.results:
            analyze = context.results['analyze']
            if analyze['success']:
                comment += f"""- **Status**: ✅ AI Analysis Complete
- **Files Analyzed**: {analyze.get('files_analyzed', 0)}
- **Total Issues Found**: {analyze.get('total_issues', 0)}
- **Overall Risk Level**: {analyze.get('overall_risk', 'UNKNOWN')}
- **Vulnerabilities**: {len(analyze.get('vulnerabilities', []))}
- **Security Issues**: {len(analyze.get('security_issues', []))}
- **Quality Issues**: {len(analyze.get('quality_issues', []))}

**🚨 Critical Issues Found:**
"""
                # Show critical vulnerabilities
                for vuln in analyze.get('vulnerabilities', [])[:3]:  # Top 3
                    if vuln.get('severity') == 'HIGH':
                        comment += f"- ⚠️ **{vuln.get('type', 'Unknown')}** (Line {vuln.get('line_number', '?')}): {vuln.get('description', 'No description')}\n"
                
                # Show recommendations
                if analyze.get('recommendations'):
                    comment += f"\n**💡 AI Recommendations:**\n"
                    for rec in analyze.get('recommendations', [])[:3]:  # Top 3
                        comment += f"- {rec}\n"
            else:
                comment += f"- **Status**: ❌ Analysis Failed\n- **Error**: {analyze.get('error', 'Unknown error')}\n"
        else:
            comment += "- **Status**: ⏭️ Skipped\n"

        comment += f"""
## 🔧 Fix Agent Results  
{context.results.get('fix', {}).get('message', 'Not executed')}

## 🧪 Test Agent Results
{context.results.get('test', {}).get('message', 'Not executed')}

---
*Powered by Hackademia Multi-Agent AI Pipeline* 🚀
"""
        
        return comment
    
    async def _send_comprehensive_results_websocket(self, context: PipelineContext, pipeline_id: str):
        """Send comprehensive results via WebSocket after pipeline completion"""
        try:
            print(f"📤 Preparing comprehensive results for WebSocket: {pipeline_id}...")
            
            # Get stored trigger information or use defaults
            trigger_info = self._pipeline_trigger_info.get(pipeline_id, {
                "trigger_type": "webhook",
                "triggered_by": "github",
                "event_type": "pull_request",
                "timestamp": time.time()
            })
            
            # Use ResultsAggregator to create comprehensive results
            comprehensive_results = ResultsAggregator.aggregate_pipeline_results(
                context, pipeline_id, trigger_info
            )
            
            # Convert to dict for WebSocket transmission
            from dataclasses import asdict
            results_dict = asdict(comprehensive_results)
            
            # Handle enums in the dict
            def serialize_enums(obj):
                if hasattr(obj, 'value'):
                    return obj.value
                elif isinstance(obj, dict):
                    return {key: serialize_enums(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [serialize_enums(item) for item in obj]
                else:
                    return obj
            
            serialized_results = serialize_enums(results_dict)
            
            # Send via WebSocket with special message type
            await self.send_websocket_message(pipeline_id, {
                "type": "pipeline_results_complete",
                "timestamp": time.time(),
                "comprehensive_results": serialized_results,
                "summary": {
                    "pipeline_id": pipeline_id,
                    "repository": context.repo_name,
                    "pr_number": context.pr_number,
                    "status": comprehensive_results.pipeline_status.value,
                    "total_duration": comprehensive_results.total_duration,
                    "success_rate": comprehensive_results.success_rate,
                    "issues_found": len(comprehensive_results.analysis_results.vulnerabilities),
                    "functions_fixed": len(comprehensive_results.fix_results.functions_fixed),
                    "tests_generated": comprehensive_results.test_results.tests_generated,
                    "tests_passed": comprehensive_results.test_results.tests_passed
                }
            })
            
            print(f"✅ Comprehensive results sent via WebSocket for {pipeline_id}")
            
            # Also save to file as backup
            webhook_sender = get_results_webhook_sender()
            webhook_sender.save_results_to_file(comprehensive_results)
            
            # Clean up trigger info
            if pipeline_id in self._pipeline_trigger_info:
                del self._pipeline_trigger_info[pipeline_id]
                
        except Exception as e:
            print(f"❌ Error sending comprehensive results via WebSocket: {str(e)}")
            # Try to save to file as fallback
            try:
                webhook_sender = get_results_webhook_sender()
                trigger_info = self._pipeline_trigger_info.get(pipeline_id, {
                    "trigger_type": "webhook", "triggered_by": "github", "event_type": "pull_request", "timestamp": time.time()
                })
                comprehensive_results = ResultsAggregator.aggregate_pipeline_results(
                    context, pipeline_id, trigger_info
                )
                webhook_sender.save_results_to_file(comprehensive_results)
            except Exception as save_error:
                print(f"❌ Failed to save results to file: {str(save_error)}")
            finally:
                # Clean up trigger info even on error
                if pipeline_id in self._pipeline_trigger_info:
                    del self._pipeline_trigger_info[pipeline_id]
    
    def get_pipeline_status(self, pipeline_id: str) -> Dict[str, Any]:
        """Get current status of a pipeline"""
        if pipeline_id not in self.active_pipelines:
            return {"error": "Pipeline not found"}
        
        context = self.active_pipelines[pipeline_id]
        return {
            "pipeline_id": pipeline_id,
            "stage": context.stage.value,
            "pr_number": context.pr_number,
            "repo_name": context.repo_name,
            "duration": time.time() - context.start_time,
            "results": context.results,
            "errors": context.errors
        }

# Global pipeline orchestrator
pipeline_orchestrator = MultiAgentPipeline()

def get_pipeline_orchestrator() -> MultiAgentPipeline:
    """Get the global pipeline orchestrator"""
    return pipeline_orchestrator
