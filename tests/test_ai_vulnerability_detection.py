#!/usr/bin/env python3
"""
Test Real AI Analysis with Vulnerable Code
This tests if our Gemini AI can detect real security vulnerabilities
"""

import os
import sys
import json
from dotenv import load_dotenv

# Load environment
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))

# Add project root to path
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from src.agents.analyze_agent import get_analyze_agent

async def test_vulnerability_detection():
    """Test AI analysis with intentionally vulnerable code"""
    
    print("ğŸ§ª TESTING REAL AI VULNERABILITY DETECTION")
    print("=" * 60)
    
    # Create analyze agent
    analyze_agent = get_analyze_agent()
    print("âœ… Analyze Agent created with Gemini AI")
    
    # Create mock diff data with vulnerable code
    vulnerable_code_lines = [
        {
            "line_number": 15,
            "content": "def vulnerable_login(username, password):",
            "type": "addition"
        },
        {
            "line_number": 16,
            "content": "    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"",
            "type": "addition"
        },
        {
            "line_number": 17,
            "content": "    return execute_query(query)",
            "type": "addition"
        },
        {
            "line_number": 25,
            "content": "def vulnerable_command_execution(user_input):",
            "type": "addition"
        },
        {
            "line_number": 26,
            "content": "    result = subprocess.run(user_input, shell=True, capture_output=True)",
            "type": "addition"
        },
        {
            "line_number": 35,
            "content": "    self.secret_key = \"hardcoded_secret_123\"",
            "type": "addition"
        },
        {
            "line_number": 40,
            "content": "    result = eval(data)",
            "type": "addition"
        }
    ]
    
    mock_diff_data = {
        "pr_info": {
            "number": 999,
            "title": "Add vulnerable authentication system",
            "base_branch": "main",
            "head_branch": "feature/vulnerable-code"
        },
        "changed_files": [
            {
                "filename": "vulnerable_test_file.py",
                "status": "added",
                "file_extension": "py",
                "is_binary": False,
                "added_lines": vulnerable_code_lines,
                "removed_lines": [],
                "context_lines": [
                    {
                        "line_number": 14,
                        "content": "import sqlite3",
                        "type": "context"
                    },
                    {
                        "line_number": 24,
                        "content": "import subprocess",
                        "type": "context"
                    }
                ]
            }
        ],
        "total_additions": len(vulnerable_code_lines),
        "total_deletions": 0
    }
    
    mock_build_context = {
        "metadata": {
            "project_type": "python",
            "total_files": 1
        },
        "dependencies": ["sqlite3", "subprocess"],
        "success": True
    }
    
    print(f"ğŸ” Testing analysis on {len(vulnerable_code_lines)} vulnerable code lines...")
    
    # Perform real AI analysis
    result = await analyze_agent.analyze_pr_diff(
        diff_data=mock_diff_data,
        build_context=mock_build_context
    )
    
    print(f"\nğŸ“Š ANALYSIS RESULTS:")
    print(f"âœ… Success: {result.success}")
    print(f"ğŸ“‚ Files Analyzed: {result.files_analyzed}")
    print(f"ğŸš¨ Total Issues: {result.total_issues}")
    print(f"ğŸ¯ Overall Risk: {result.overall_risk}")
    
    print(f"\nğŸ”’ SECURITY VULNERABILITIES ({len(result.vulnerabilities)}):")
    for i, vuln in enumerate(result.vulnerabilities, 1):
        print(f"   {i}. {vuln.get('type', 'Unknown')} - {vuln.get('severity', 'Unknown')}")
        print(f"      Line {vuln.get('line_number', 'N/A')}: {vuln.get('description', 'No description')[:80]}...")
    
    print(f"\nğŸ›¡ï¸ SECURITY ISSUES ({len(result.security_issues)}):")
    for i, issue in enumerate(result.security_issues, 1):
        print(f"   {i}. {issue.get('type', 'Unknown')} - {issue.get('severity', 'Unknown')}")
        print(f"      Line {issue.get('line_number', 'N/A')}: {issue.get('description', 'No description')[:80]}...")
    
    print(f"\nğŸ“ QUALITY ISSUES ({len(result.quality_issues)}):")
    for i, quality in enumerate(result.quality_issues, 1):
        print(f"   {i}. {quality.get('type', 'Unknown')} - {quality.get('severity', 'Unknown')}")
        print(f"      Line {quality.get('line_number', 'N/A')}: {quality.get('description', 'No description')[:80]}...")
    
    print(f"\nğŸ’¡ RECOMMENDATIONS ({len(result.recommendations)}):")
    for i, rec in enumerate(result.recommendations, 1):
        print(f"   {i}. {rec[:100]}...")
    
    # Check if AI detected the vulnerabilities we expected
    expected_vulnerabilities = ['SQL_INJECTION', 'COMMAND_INJECTION', 'CODE_INJECTION']
    detected_types = [v.get('type', '') for v in result.vulnerabilities]
    detected_types.extend([s.get('type', '') for s in result.security_issues])
    
    print(f"\nğŸ¯ VULNERABILITY DETECTION TEST:")
    for expected in expected_vulnerabilities:
        if any(expected in detected for detected in detected_types):
            print(f"   âœ… {expected} - DETECTED")
        else:
            print(f"   âŒ {expected} - NOT DETECTED")
    
    if result.total_issues > 0:
        print(f"\nğŸ‰ SUCCESS: AI detected {result.total_issues} security issues!")
        print("âœ… The Analyze Agent is using REAL Gemini AI analysis!")
    else:
        print(f"\nâš ï¸ WARNING: No issues detected - AI might not be analyzing correctly")
    
    return result

if __name__ == "__main__":
    import asyncio
    
    try:
        result = asyncio.run(test_vulnerability_detection())
        print(f"\nâœ… Test completed - Overall Risk: {result.overall_risk}")
    except Exception as e:
        print(f"âŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
